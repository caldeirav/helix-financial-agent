# Environment Configuration for Helix Financial Agent
# Copy this file to .env and fill in your values

# ============================================================================
# HARDWARE CONFIGURATION
# ============================================================================
DEVICE=cuda
CUDA_VISIBLE_DEVICES=0

# ============================================================================
# MODEL SERVING - llama.cpp
# ============================================================================
LLAMA_CPP_BASE_URL=http://localhost:8081/v1
LLAMA_CPP_API_KEY=not-needed
AGENT_MODEL_NAME=qwen3-30b-a3b-instruct-2507

# Model parameters
AGENT_TEMPERATURE=0.7
AGENT_TOP_P=0.8
AGENT_MAX_TOKENS=4096

# ============================================================================
# SEMANTIC ROUTER - vLLM-SR
# ============================================================================
# vLLM-SR ports per documentation (https://vllm-semantic-router.com/docs/api/router):
# - 8801: HTTP entry point through Envoy (OpenAI-compatible API)
# - 8080 (container) → ROUTER_CLASSIFY_PORT: Classification API (health, /v1/models)
# - 9190: Prometheus metrics
# - 8700: Hub UI dashboard (vLLM-SR DASHBOARD_PORT; used by port-forward script)
ROUTER_HOST=localhost
ROUTER_HTTP_PORT=8801
ROUTER_CLASSIFY_PORT=8889
ROUTER_METRICS_PORT=9190
ROUTER_HUB_PORT=8700
ROUTER_ENDPOINT=http://localhost:8801/v1

# ============================================================================
# JUDGE MODEL (LLM-as-a-Judge evaluation)
# ============================================================================
# Get your API key from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here
# Best quality: gemini-2.5-pro. Faster/cheaper: gemini-2.5-flash
GEMINI_MODEL=gemini-2.5-pro

# ============================================================================
# MCP SERVER CONFIGURATION
# ============================================================================
MCP_SERVER_HOST=localhost
MCP_SERVER_PORT=8000
MCP_TRANSPORT=streamable-http

# ============================================================================
# TOOL RAG CONFIGURATION
# ============================================================================
# ToolRAG (Tool Retrieval Augmented Generation) dynamically selects relevant
# tools for each query using semantic similarity search.
#
# EMBEDDING_MODEL: Model for computing query/tool embeddings
#   Default: sentence-transformers/all-MiniLM-L6-v2 (fast, good quality)
#
# TOOL_RAG_TOP_K: Max tools to retrieve from vector search
#   Note: This is initial retrieval, further filtering by threshold follows
#
# TOOL_RAG_THRESHOLD: Minimum similarity score (0.0-1.0) for tool selection
#   Lower = more tools selected (may include irrelevant)
#   Higher = fewer tools (may miss relevant ones)
#   Recommended: 0.3-0.4 for good balance
#
# TOOL_RAG_MAX_TOOLS: Maximum tools to pass to LLM (prevents context overflow)
#   Each tool adds ~1000-2000 tokens to prompt. With 16K context:
#   - max_tools=3 → ~4.5K tokens for tools (recommended for 16K models)
#   - max_tools=4 → ~6K tokens (may overflow with metacognitive iterations)
#   - Without limit, 13 tools × 1500 = ~20K tokens (exceeds 16K!)
#   Keep conservative since metacognitive loop adds context per iteration.
#
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
TOOL_RAG_TOP_K=5
TOOL_RAG_THRESHOLD=0.3
TOOL_RAG_MAX_TOOLS=3

# ============================================================================
# WEB UI PORTS (server – where services listen)
# ============================================================================
# Used when starting Streamlit, MLflow UI, and by the router Hub. Same .env
# is used on the server and (for remote port in forwarding) on the local machine.
STREAMLIT_PORT=8501
MLFLOW_PORT=5000
# ROUTER_HUB_PORT is defined under SEMANTIC ROUTER (default 8700)

# ============================================================================
# PORT FORWARDING (local machine only)
# ============================================================================
# Used by scripts/ssh_port_forward.sh on your LOCAL machine: local bind ports
# for SSH tunnels. Remote (target) ports are STREAMLIT_PORT, MLFLOW_PORT,
# ROUTER_HUB_PORT from above.
LOCAL_STREAMLIT_PORT=8501
LOCAL_MLFLOW_PORT=5000
LOCAL_ROUTER_HUB_PORT=8700

# ============================================================================
# MLFLOW TRACING
# ============================================================================
# MLflow captures end-to-end traces of agent execution including:
# - LLM calls (generator, reflector, revisor)
# - Tool executions (via MCP server)
# - Routing decisions (semantic router)
#
# Per-trace assessments are logged:
# - tool_selection_successful: Y/N
# - model_selection_successful: Y/N
# - judge_score: 0-10
#
# View traces: mlflow ui --port 5000 → http://localhost:5000
#
# Tracking URI can be:
# - Local directory: ./mlruns (default)
# - Remote server: http://mlflow-server:5000
# - Databricks: databricks://profile_name
MLFLOW_TRACKING_URI=./mlruns
MLFLOW_EXPERIMENT_NAME=helix-financial-agent

# ============================================================================
# AGENT SETTINGS
# ============================================================================
# Max metacognitive iterations (reflect → revise → tools/reflect)
MAX_ITERATIONS=3
# Max graph steps per run (failsafe to prevent infinite tool-call loops)
MAX_AGENT_STEPS=80

# ============================================================================
# DATA PATHS
# ============================================================================
DATA_DIR=./data
LOG_DIR=./logs
LOG_LEVEL=INFO

# ============================================================================
# HUGGINGFACE (for model downloads)
# ============================================================================
# HF_TOKEN is used for:
# - Downloading embedding models (sentence-transformers)
# - Accessing gated models on HuggingFace Hub
# - Higher rate limits and faster downloads
#
# Get your token from: https://huggingface.co/settings/tokens
# Without a token, you may see warnings like:
#   "Warning: You are sending unauthenticated requests to the HF Hub"
#
# The token is automatically picked up by the ToolStore when loading
# the embedding model for semantic tool selection.
#
HF_TOKEN=your_huggingface_token_here
HF_ENDPOINT=https://huggingface.co
